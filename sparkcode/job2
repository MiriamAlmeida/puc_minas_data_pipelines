from pyspark.sql import functions as f
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .config("spark.jars.packages", "io.delta:delta-core_2.12:2.1.0")
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

from delta.tables import *

print("LENDO CSV do S3...")

media_tot = spark.read.csv(
    "s3://miriam-341567884088/indicadores_media_total/",
    header=True, inferSchema=True, sep=";"
)

media_mulheres = spark.read.csv(
    "s3://miriam-341567884088/indicadores_media_total_mulheres/", 
    header=True, inferSchema=True, sep=";"
)

functools.reduce(lambda media_tot, media_mulheres: media_tot.union(media_mulheres.select(media_tot.columns)), [media_tot, media_mulheres])

(
    query_media_total
    .write
    .format('parquet')
    .save('s3://miriam-341567884088/indicadores_media_total/')
)